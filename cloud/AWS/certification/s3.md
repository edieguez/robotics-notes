# Simple Storage Service (S3)

Is one of the main building blocks in AWS. It has *infinite storage* capacity

- It stores *objects* in *buckets*
- There is no concept of *file* and *directory*
  - Although the web interface may trick you into thinking that
- Buckets must have a **globally unique name** across all regions and accounts
  - Although buckets are defined at region level
- It supports encryption as part of its security features

## Storage classes

You can move between classes manually or using S3 Lifecycle configurations

1. `Standard` general purpose
2. `Standard-Infrequent Access (IA)` for data that is accessed less frequently
3. `One Zone-Infrequent Access` for data that is accessed less frequently but don't need the redundancy
4. `Glaciar Instant Retrieval` long term archive with instant retrieval
5. `Glaciar Flexible Retrieval (formerly know as Amazon S3 Glacier)`
   1. `Expedited` 1 to 5 minutes
   2. `Standard` 3 to 5 hours
   3. `Bulk` 5 to 12 hours
6. `Glaciar Deep Archive` long term archive with 12 hours retrieval
7. `Intelligent Tiering` automatically moves objects between two access tiers based on changing access patterns

## Transfer acceleration

- Uses the CloudFront Edge Network to accelerate uploads to S3
- Only get charged if the acceleration happens

## Byte-range fetches

- Parallelize GETs by requesting specific byte ranges
- Better resilience in case of failures
- Can be used to speed up downloads
- Can be used to get only partial data

## Naming convention

- No uppercase
- No underscore
- 3 to 63 character long
- Not an IP
- Must start with lowercase letter or number
- Must not start with the prefix `xn--`
- Must not end with the suffix `-s3alias`

## Objects

- Object values are the content of the body
- Max size is 5TB
  - If uploading more than 5GB, must use *multi-part upload*
- Metadata is a set of name-value pairs
  - Can be created by the system or user
- Supports tags
  - Up to 10 tags per object
- Version ID
  - If versioning is enabled, each object has a unique version ID

## S3 use cases

- Backup and storage
- Disaster recovery
- Archive
- Hybrid Cloud storage
- Media hosting
- Data lakes and big data analytics
- Software delivery
- Static website

## Security: bucket policy

### User based

- `IAM policies` which API calls should be allowed for a specific user from IAM

### Resource based

- `Bucket policies` bucket wide rules from the S3 console - **most common, allows cross account**
- `Object Access Control List (ACL)` can be used to manage access to individual objects - can be disabled
- `Bucket Access Control List (ACL)` can be used to manage access to the bucket - less common, can be disabled

#### IAM principal

It can access an S3 object if

- The use IAM permission ALLOW it or the resource policy ALLOWS it
- There is no explicit DENY

## Bucket policies

### JSON based

- `Resources` bucket and objects
- `Effect` Allow or Deny
- `Actions` set of actions to allow or deny
- `Principal` account or user to apply the policy to

```json
{
  "Version": "2012-10-17",
  "Statement": [
    "Sid": "PublicRead",
    "Effect": "Allow",
    "Principal": "*",
    "Action": [
      "s3:GetObject"
    ],
    "Resource": "arn:aws:s3:::mybucket/*"
  ]
}
```

#### Bucket policies use cases

- Grant public access to the bucket
  - There are some special rules for blocking public access
  - These were created to prevent data leaks
  - This blocking can be set at account level
- Force objects to be encrypted at upload
- Grant access to another account (**cross account**)

## Static website hosting

- Can host websites and have them accessible from the internet
- The website URL will depend on the bucket name and region
- The bucket policy must allow public access
  - You will notice this if you get a `403 error`

## Replication (CRR and SRR)

> `CRR` Cross Region Replication
> `SRR` Same Region Replication

- Versioning must be enabled on both the source and destination
- Buckets can be in different AWS accounts
- Copying is asynchronous
- Must give proper IAM permissions to S3 to replicate objects

### Replication considerations

- **After you enable replication, only new objects are replicated**
- You can replicate existing objects using `S3 Batch Replication`
- For DELETE operations
  - Can replicate delete markers from source to target (optional setting)
  - Deletions with a version ID are not replicated (to void malicious deletes)
- **There is no "chaining" replication**
  - If bucket 1 has replication into bucket 2, which has replication into bucket3
  - The objects created in bucket 1 are not replicated to bucket 3

## Resources

- [AWS policy generator](https://awspolicygen.s3.amazonaws.com/policygen.html)
- [Object Storage Classes](https://aws.amazon.com/s3/storage-classes)
